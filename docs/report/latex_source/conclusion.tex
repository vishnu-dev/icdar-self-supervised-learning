\chapter{Conclusion}

Self-supervised image representation learning using deep convolutional neural networks and transformers for unlabelled data has shown great success. This project has extensively experimented on reviewing some of the state-of-the-art Self-supervised learning techniques on the \gls{icdar} \gls{clamm} dataset. Under the linear evaluation protocol on the dataset, \gls{simclr} performs the best among \gls{mae} and \gls{byol}, while using comparatively fewer parameters. In order to enhance accuracies, it is imperative to conduct further research and experimentation to delve into the intricacies of \gls{mae} and \gls{byol} methodologies. Nevertheless, this study will be the foundation for implementing Self-supervised techniques for learning good representations in ancient handwritten script data.