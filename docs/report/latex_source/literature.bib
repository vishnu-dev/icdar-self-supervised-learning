@article{najafabadi_deep_2015,
	title = {Deep learning applications and challenges in big data analytics},
	volume = {2},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-014-0007-7},
	doi = {10.1186/s40537-014-0007-7},
	abstract = {Big Data Analytics and Deep Learning are two high-focus of data science. Big Data has become important as many organizations both public and private have been collecting massive amounts of domain-specific information, which can contain useful information about problems such as national intelligence, cyber security, fraud detection, marketing, and medical informatics. Companies such as Google and Microsoft are analyzing large volumes of data for business analysis and decisions, impacting existing and future technology. Deep Learning algorithms extract high-level, complex abstractions as data representations through a hierarchical learning process. Complex abstractions are learnt at a given level based on relatively simpler abstractions formulated in the preceding level in the hierarchy. A key benefit of Deep Learning is the analysis and learning of massive amounts of unsupervised data, making it a valuable tool for Big Data Analytics where raw data is largely unlabeled and un-categorized. In the present study, we explore how Deep Learning can be utilized for addressing some important problems in Big Data Analytics, including extracting complex patterns from massive volumes of data, semantic indexing, data tagging, fast information retrieval, and simplifying discriminative tasks. We also investigate some aspects of Deep Learning research that need further exploration to incorporate specific challenges introduced by Big Data Analytics, including streaming data, high-dimensional data, scalability of models, and distributed computing. We conclude by presenting insights into relevant future works by posing some questions, including defining data sampling criteria, domain adaptation modeling, defining criteria for obtaining useful data abstractions, improving semantic indexing, semi-supervised learning, and active learning.},
	number = {1},
	urldate = {2023-07-12},
	journal = {Journal of Big Data},
	author = {Najafabadi, Maryam M. and Villanustre, Flavio and Khoshgoftaar, Taghi M. and Seliya, Naeem and Wald, Randall and Muharemagic, Edin},
	month = feb,
	year = {2015},
	keywords = {Big data, Deep learning},
	pages = {1},
	file = {Full Text PDF:/Users/vishnudev/Zotero/storage/59B8AR6F/Najafabadi et al. - 2015 - Deep learning applications and challenges in big d.pdf:application/pdf;Snapshot:/Users/vishnudev/Zotero/storage/4MANMX3X/s40537-014-0007-7.html:text/html},
}


@inproceedings{cloppet_icdar2017_2017,
	title = {{ICDAR2017} {Competition} on the {Classification} of {Medieval} {Handwritings} in {Latin} {Script}},
	volume = {01},
	doi = {10.1109/ICDAR.2017.224},
	abstract = {This paper presents the results of the ICDAR2017 Competition on the Classification of Medieval Handwritings in Latin Script (CLaMM), jointly organized by Computer Scientists and Humanists (paleographers). This work follows a competition at ICFHR2016 and aims at providing a rich annotated database of European medieval manuscripts to the community on Handwriting Analysis and Recognition. We proposed four independent classification tasks which attracted 10 registered teams, with 6 submitted classifiers from 4 participants. Those classifiers are trained on a set of 3540 images with their ground truths. In task 1 (Script classification) and task 3 (Date classification), the classifiers have been evaluated by a test set of 2000 greyscale, tiff, 300 dpi images. In task 2 (Script classification) and task 4 (Date classification), the test set consists of 1000 images in different formats, resolutions and color representation. The best scores are respectively 85.2\% for task 1, 76.5\% for task 2, 59\% for task 3, and 49.9\% for task 4. An analysis based on the matrix of confusion of each classifier is also given.},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Cloppet, Florence and Eglin, Véronique and Helias-Baron, Marlène and Kieu, Cuong and Vincent, Nicole and Stutzmann, Dominique},
	month = nov,
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Character recognition, Europe, Feature extraction, Historical documents, Image classification, Image color analysis, Image resolution, Libraries, Medieval Latin script classification, Quantitative analysis, Task analysis, Training, Training data},
	pages = {1371--1376},
	file = {IEEE Xplore Abstract Record:/Users/vishnudev/Zotero/storage/MYTIU2NN/stamp.html:text/html;IEEE Xplore Full Text PDF:/Users/vishnudev/Zotero/storage/ABGI8DQZ/Cloppet et al. - 2017 - ICDAR2017 Competition on the Classification of Med.pdf:application/pdf},
}

@misc{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	urldate = {2023-06-12},
	publisher = {arXiv},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv:2002.05709 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICML'2020. Code and pretrained models at https://github.com/google-research/simclr},
	file = {arXiv.org Snapshot:/Users/vishnudev/Zotero/storage/AKBTGDM3/2002.html:text/html;Full Text PDF:/Users/vishnudev/Zotero/storage/5G8NVN5A/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf},
}


@misc{he_masked_2021,
	title = {Masked {Autoencoders} {Are} {Scalable} {Vision} {Learners}},
	url = {http://arxiv.org/abs/2111.06377},
	doi = {10.48550/arXiv.2111.06377},
	abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.},
	urldate = {2023-06-17},
	publisher = {arXiv},
	author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
	month = dec,
	year = {2021},
	note = {arXiv:2111.06377 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report. arXiv v2: add more transfer learning results; v3: add robustness evaluation},
	file = {arXiv Fulltext PDF:/Users/vishnudev/Zotero/storage/PFGDEDGY/He et al. - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/vishnudev/Zotero/storage/JE2LNVK7/2111.html:text/html},
}


@inproceedings{grill_bootstrap_2020,
	title = {Bootstrap {Your} {Own} {Latent} - {A} {New} {Approach} to {Self}-{Supervised} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc//paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html},
	abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods intrinsically rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches 74.3\% top-1 classification accuracy on ImageNet using the standard linear evaluation protocol with a standard ResNet-50 architecture and 79.6\% with a larger ResNet. We also show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks.},
	urldate = {2023-06-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and Piot, Bilal and kavukcuoglu, koray and Munos, Remi and Valko, Michal},
	year = {2020},
	pages = {21271--21284},
}


@article{ericsson_self-supervised_2022,
	title = {Self-{Supervised} {Representation} {Learning}: {Introduction}, advances, and challenges},
	volume = {39},
	issn = {1558-0792},
	shorttitle = {Self-{Supervised} {Representation} {Learning}},
	doi = {10.1109/MSP.2021.3134634},
	abstract = {Self-supervised representation learning (SSRL) methods aim to provide powerful, deep feature learning without the requirement of large annotated data sets, thus alleviating the annotation bottleneck—one of the main barriers to the practical deployment of deep learning today. These techniques have advanced rapidly in recent years, with their efficacy approaching and sometimes surpassing fully supervised pretraining alternatives across a variety of data modalities, including image, video, sound, text, and graphs. This article introduces this vibrant area, including key concepts, the four main families of approaches and associated state-of-the-art techniques, and how self-supervised methods are applied to diverse modalities of data. We further discuss practical considerations including workflows, representation transferability, and computational cost. Finally, we survey major open challenges in the field, that provide fertile ground for future work.},
	number = {3},
	journal = {IEEE Signal Processing Magazine},
	author = {Ericsson, Linus and Gouk, Henry and Loy, Chen Change and Hospedales, Timothy M.},
	month = may,
	year = {2022},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {Annotations, Computational efficiency, Deep learning, Representation learning, Self-supervised learning},
	pages = {42--62},
	file = {IEEE Xplore Abstract Record:/Users/vishnudev/Zotero/storage/S33JW8LU/9770283.html:text/html;IEEE Xplore Full Text PDF:/Users/vishnudev/Zotero/storage/QGCKZLYZ/Ericsson et al. - 2022 - Self-Supervised Representation Learning Introduct.pdf:application/pdf},
}


@misc{noauthor_icdar2017-clamm_nodate,
	title = {{ICDAR2017}-{CLaMM}},
	url = {https://clamm.irht.cnrs.fr/icdar-2017/icdar2017-clamm/},
	abstract = {ICDAR2017 Competition on the Classification of Medieval Handwritings in Latin Script UPDATE (1/03/2017) The Training Data Set for the ICDAR2017 CLAMM competition is available. Please register (how to is here: Registration) The proposed competition is the second round of the Competition on the Classification of Medieval Handwritings in Latin Script. The first round was organized … Continuer la lecture de ICDAR2017-CLaMM →},
        author = {ICDAR},
	language = {fr-FR},
        year = {2017},
	urldate = {2023-05-29},
	journal = {Classification of Medieval Handwritings in Latin Script},
	file = {Snapshot:/Users/vishnudev/Zotero/storage/M6LRQS8H/icdar2017-clamm.html:text/html},
}

@Misc{Yadan2019Hydra,
  author =       {Omry Yadan},
  title =        {Hydra - A framework for elegantly configuring complex applications},
  howpublished = {Github},
  year =         {2019},
  url =          {https://github.com/facebookresearch/hydra}
}

@misc{Falcon2019,
  author = {Falcon, W.A. et al.},
  title = {PyTorch Lightning},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/PytorchLightning/pytorch-lightning}}
}


@inproceedings{kocaman_saliency_2022,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Saliency {Can} {Be} {All} {You} {Need} in {Contrastive} {Self}-supervised {Learning}},
	isbn = {978-3-031-20716-7},
	doi = {10.1007/978-3-031-20716-7_10},
	abstract = {We propose an augmentation policy for Contrastive Self-Supervised Learning (SSL) in the form of an already established Salient Image Segmentation technique entitled Global Contrast based Salient Region Detection. This detection technique, which had been devised for unrelated Computer Vision tasks, was empirically observed to play the role of an augmentation facilitator within the SSL protocol. This observation is rooted in our practical attempts to learn, by SSL-fashion, aerial imagery of solar panels, which exhibit challenging boundary patterns. Upon the successful integration of this technique on our problem domain, we formulated a generalized procedure and conducted a comprehensive, systematic performance assessment with various Contrastive SSL algorithms subject to standard augmentation techniques. This evaluation, which was conducted across multiple datasets, indicated that the proposed technique indeed contributes to SSL. We hypothesize whether salient image segmentation may suffice as the only augmentation policy in Contrastive SSL when treating downstream segmentation tasks.},
	language = {en},
	booktitle = {Advances in {Visual} {Computing}},
	publisher = {Springer Nature Switzerland},
	author = {Kocaman, Veysel and Shir, Ofer M. and Bäck, Thomas and Belbachir, Ahmed Nabil},
	editor = {Bebis, George and Li, Bo and Yao, Angela and Liu, Yang and Duan, Ye and Lau, Manfred and Khadka, Rajiv and Crisan, Ana and Chang, Remco},
	year = {2022},
	pages = {119--140},
	file = {Full Text PDF:/Users/vishnudev/Zotero/storage/38NU2SR7/Kocaman et al. - 2022 - Saliency Can Be All You Need in Contrastive Self-s.pdf:application/pdf},
}


@article{maaten_visualizing_2008,
	title = {Visualizing {Data} using t-{SNE}},
	volume = {9},
	url = {http://jmlr.org/papers/v9/vandermaaten08a.html},
	number = {86},
	journal = {Journal of Machine Learning Research},
	author = {Maaten, Laurens van der and Hinton, Geoffrey},
	year = {2008},
	pages = {2579--2605},
}
