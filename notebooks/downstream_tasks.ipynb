{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating downstream model against pre-trained Self-Supervised models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T17:08:35.256482447Z",
     "start_time": "2023-04-24T17:08:28.225101976Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:35: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:93: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pl_bolts/losses/self_supervised_learning.py:234: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n",
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pl_bolts/datamodules/experience_source.py:18: UnderReviewWarning: The feature warn_missing_pkg is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  warn_missing_pkg(\"gym\")\n"
     ]
    }
   ],
   "source": [
    "from pl_bolts.models.self_supervised.simclr import SimCLREvalDataTransform\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from pl_bolts.models.self_supervised import SimCLR\n",
    "import torchvision.transforms as T\n",
    "from lightly.models import utils\n",
    "from lightly.models.modules import masked_autoencoder\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import plotly.express as px\n",
    "import pytorch_lightning as pl\n",
    "from time import time\n",
    "from PIL import Image\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from src.models.mae.model import MAE\n",
    "from src.models.byol.model import BYOL\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_best_checkpoint(selected_model, choice=None):\n",
    "    logs_dir = os.path.join(\n",
    "        '/home/woody/iwfa/iwfa028h/dev/faps/data/trained_models/',\n",
    "        selected_model,\n",
    "        'lightning_logs'\n",
    "    )\n",
    "\n",
    "    best_version = max(\n",
    "        map(\n",
    "            lambda x: int(x.replace('version_', '')) if 'version' in x else 0,\n",
    "            os.listdir(logs_dir)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    version_dir = os.path.join(logs_dir, f'version_{best_version if not choice else choice}', 'checkpoints')\n",
    "    best_checkpoint = os.path.join(version_dir, os.listdir(version_dir)[0])\n",
    "    print('LATEST CHECKPOINT', best_checkpoint)\n",
    "\n",
    "    return best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_best_checkpoint(model_name, choice=None, **model_kwargs):\n",
    "    \n",
    "    checkpoint = get_best_checkpoint(model_name, choice)\n",
    "    print(model_kwargs)\n",
    "    \n",
    "    if model_name == 'SimCLR':\n",
    "        model = SimCLR.load_from_checkpoint(checkpoint, strict=False, **model_kwargs)\n",
    "        return model.encoder\n",
    "    elif model_name == 'BYOL':\n",
    "        model = BYOL.load_from_checkpoint(checkpoint, strict=False, **model_kwargs)\n",
    "        return model\n",
    "    elif model_name == 'MAE':\n",
    "        model = MAE.load_from_checkpoint(checkpoint, strict=False, **model_kwargs)\n",
    "        return model\n",
    "    elif model_name in ['SimCLRDownstream', 'MAEDownstream', 'BYOLDownstream', 'DownstreamClassifier']:\n",
    "        model = DownstreamClassifier.load_from_checkpoint(checkpoint, strict=False, **model_kwargs)\n",
    "        return model\n",
    "    else:\n",
    "        model = SimCLR.load_from_checkpoint(checkpoint, strict=False)\n",
    "        return embeddings_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ICDARDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_filepath, root_dir, transforms=None, convert_rgb=True):\n",
    "        \n",
    "        self.transforms = transforms\n",
    "        self.convert_rgb = convert_rgb\n",
    "        \n",
    "        df = pd.read_csv(csv_filepath, sep=';')\n",
    "        df['img_path'] = root_dir + os.sep + df.FILENAME\n",
    "        self.data = df.loc[\n",
    "            (df.img_path.map(os.path.exists)) &\n",
    "            (df.img_path.str.contains(''))\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = self.data.loc[idx, 'img_path']\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "        except Exception as ex:\n",
    "            return None\n",
    "\n",
    "        if self.convert_rgb:\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        return image, self.data.loc[idx, 'SCRIPT_TYPE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_factory(dataset_name, root_dir, label_filepath, transforms, mode, batch_size, collate_fn=None, num_cpus=None):\n",
    "\n",
    "    if dataset_name.lower() == 'icdar':\n",
    "        dataset = ICDARDataset(label_filepath, root_dir, transforms=transforms(), convert_rgb=True)\n",
    "    else:\n",
    "        raise NotImplementedError(f'Dataset {dataset_name} is not implemented')\n",
    "\n",
    "    total_count = len(dataset)\n",
    "    train_count = int(0.7 * total_count)\n",
    "    val_count = int(0.1 * total_count)\n",
    "    test_count = total_count - train_count - val_count\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset,\n",
    "        (train_count, val_count, test_count),\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    if mode in 'train':\n",
    "        return {\n",
    "            'train': DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                drop_last=True,\n",
    "                pin_memory=True,\n",
    "#                 persistent_workers=False,\n",
    "                num_workers=num_cpus or os.cpu_count(),\n",
    "                collate_fn=collate_fn() if collate_fn else None\n",
    "            ),\n",
    "            'val': DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                drop_last=False,\n",
    "                pin_memory=True,\n",
    "#                 persistent_workers=False,\n",
    "                num_workers=num_cpus or os.cpu_count(),\n",
    "                collate_fn=collate_fn() if collate_fn else None\n",
    "            )\n",
    "        }\n",
    "    elif mode == 'test':\n",
    "        return {\n",
    "            'test': DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                drop_last=False,\n",
    "                pin_memory=True,\n",
    "#                 persistent_workers=False,\n",
    "                num_workers=num_cpus or os.cpu_count(),\n",
    "                collate_fn=collate_fn() if collate_fn else None\n",
    "            )\n",
    "        }\n",
    "    else:\n",
    "        raise KeyError(f'Unknown mode: {mode}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# root_dir = os.path.join(\n",
    "#     '/home/woody/iwfa/iwfa028h/dev/faps', 'data', 'ICDAR2017_CLaMM_Training'\n",
    "# )\n",
    "\n",
    "# train_dataloaders = data_factory(\n",
    "#     dataset_name='icdar',\n",
    "#     root_dir=root_dir,\n",
    "#     label_filepath=os.path.join(root_dir, '@ICDAR2017_CLaMM_Training.csv'),\n",
    "#     transforms=SimCLREvalDataTransform,\n",
    "#     mode='train',\n",
    "#     batch_size=256,\n",
    "#     num_cpus=4\n",
    "# )\n",
    "\n",
    "# test_dataloaders = data_factory(\n",
    "#     dataset_name='icdar',\n",
    "#     root_dir=root_dir, \n",
    "#     label_filepath=os.path.join(root_dir, '@ICDAR2017_CLaMM_Training.csv'),\n",
    "#     transforms=SimCLREvalDataTransform,\n",
    "#     mode='test',\n",
    "#     batch_size=256,\n",
    "#     num_cpus=4\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_features(model, data_loader, num_feats, batch_size, num_samples, perplexity=25):\n",
    "    num_samples = len(data_loader) if not num_samples else num_samples\n",
    "    feats = np.array([]).reshape((0, num_feats))\n",
    "    labels = np.array([])\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "\n",
    "    processed_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for (x1, x2, _), label in data_loader:\n",
    "            if processed_samples >= num_samples:\n",
    "                break\n",
    "            x1 = x1.squeeze().cuda()\n",
    "            out = model(x1)\n",
    "            out = out[-1].detach().cpu().numpy()\n",
    "            print(out.shape)\n",
    "            feats = np.append(feats, out, axis=0)\n",
    "            labels = np.append(labels, label, axis=0)\n",
    "            processed_samples += batch_size\n",
    "\n",
    "    tsne = TSNE(n_components=3, perplexity=perplexity, init='pca')\n",
    "    x_feats = tsne.fit_transform(feats)\n",
    "\n",
    "    dim_red_df = pd.DataFrame(x_feats)\n",
    "    dim_red_df['labels'] = pd.Categorical(labels)\n",
    "    fig = px.scatter_3d(dim_red_df, x=0, y=1, z=2, color='labels', size_max=5)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_from_embeddings(model, dataloader):\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for images, labels in dataloader:\n",
    "        x1, x2, _ = images\n",
    "        x1 = x1.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(x1)[-1].detach().cpu().numpy()\n",
    "        X.append(embeddings)\n",
    "        y.append(labels.numpy())\n",
    "        \n",
    "    X = np.concatenate(X)\n",
    "    y = np.concatenate(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cluster_embeddings():\n",
    "    \n",
    "    simclr_encoder = load_best_checkpoint('SimCLR')\n",
    "    simclr_encoder.eval()\n",
    "    simclr_encoder.cuda()\n",
    "    \n",
    "    X_train, y_train = generate_from_embeddings(simclr_encoder, train_dataloaders.get('train'))\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=10)\n",
    "    print(knn)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    X_test, y_test = generate_from_embeddings(simclr_encoder, test_dataloaders.get('test'))\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    return knn\n",
    "    \n",
    "#     plot_features(simclr_encoder, dataloaders.get('train'), 2048, 256, 1000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear evaluation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DownstreamClassifier(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, base_model_name='SimCLR', base_model_version=None, features=2048, num_classes=13, learning_rate=3e-4):\n",
    "        print(base_model_name, features, num_classes)\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "                \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        s = time()\n",
    "        self.base_model = load_best_checkpoint(base_model_name, choice=base_model_version, num_classes=num_classes)\n",
    "        self.base_model.eval()\n",
    "        self.base_model.cuda()\n",
    "        print(self.base_model)\n",
    "        print('Base model load time: ', time() - s)\n",
    "\n",
    "        self.classifier = nn.Linear(features, num_classes)\n",
    "        \n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.accuracy_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.num_classes).to(self.device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.base_model(x, return_embedding=True, return_projection=False)\n",
    "            # print(x.shape)\n",
    "            # x = self.base_model.forward_encoder(x, 0)\n",
    "            # if isinstance(x, list):\n",
    "            #     x = x[-1]\n",
    "            # elif isinstance(x, tuple):\n",
    "            #     x = x[0]\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, label = batch\n",
    "        y_hat = self(images)\n",
    "        # y_hat, _ = torch.max(y_hat, dim=1)\n",
    "        acc = self.accuracy_fn(y_hat, label)\n",
    "        self.log('train_accuracy', acc, prog_bar=True)\n",
    "        loss = self.loss_fn(y_hat, label)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, label = batch\n",
    "        y_hat = self(images)\n",
    "        # y_hat, _ = torch.max(y_hat, dim=1)\n",
    "        loss = self.loss_fn(y_hat, label)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, label = batch\n",
    "        y_hat = self(images)\n",
    "        y_hat, _ = torch.max(y_hat, dim=1)\n",
    "        acc = self.accuracy_fn(y_hat, label)\n",
    "        self.log('test_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=0.0008)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_downstream_model(model_name, downstream_model_name, model_version=None, feats=2048, max_epochs=10):\n",
    "    \n",
    "    root_dir = os.path.join(\n",
    "        '/home/woody/iwfa/iwfa028h/dev/faps', 'data', 'ICDAR2017_CLaMM_Large'\n",
    "    )\n",
    "    dataloaders = data_factory(\n",
    "        dataset_name='icdar',\n",
    "        root_dir=root_dir, \n",
    "        label_filepath=os.path.join(root_dir, '@ICDAR2017_CLaMM_Large.csv'),\n",
    "        transforms=SimCLREvalDataTransform,\n",
    "        mode='train',\n",
    "        batch_size=64,\n",
    "        num_cpus=8\n",
    "    )\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=os.path.abspath(os.path.join(root_dir, '..', 'trained_models', downstream_model_name)),\n",
    "        accelerator='gpu',\n",
    "        devices=-1,\n",
    "        max_epochs=max_epochs,\n",
    "        enable_progress_bar=True,\n",
    "        precision=16,\n",
    "        callbacks=[\n",
    "            pl.callbacks.ModelCheckpoint(mode=\"min\", monitor=\"val_loss\"),\n",
    "            pl.callbacks.RichProgressBar()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    downstream_classifier = DownstreamClassifier(model_name, model_version, feats, 13)\n",
    "    \n",
    "    trainer.fit(downstream_classifier, dataloaders.get('train'), dataloaders.get('val'))\n",
    "    \n",
    "    return downstream_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Epoch 49/49 <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">69/69</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:01:59 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">0.76it/s</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">loss: 1.81 v_num: 617786           </span>\n",
       "                                                                                <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">train_accuracy: 0.375              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Epoch 49/49 \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m69/69\u001b[0m \u001b[38;5;245m0:01:59 • 0:00:00\u001b[0m \u001b[38;5;249m0.76it/s\u001b[0m \u001b[37mloss: 1.81 v_num: 617786           \u001b[0m\n",
       "                                                                                \u001b[37mtrain_accuracy: 0.375              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# downstream_model = train_downstream_model('SimCLR', 'SimCLRDownstream', None, 2048, 100)\n",
    "# downstream_model = train_downstream_model('MAE', 'MAEDownstream', '614645', 1024, 100)\n",
    "downstream_model = train_downstream_model('BYOL', 'BYOLDownstream', '617770', 2048, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the linear model on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_downstream_model(base_model_name, model_name, num_features, base_model_version):\n",
    "    \n",
    "    root_dir = os.path.join(\n",
    "        '/home/woody/iwfa/iwfa028h/dev/faps', 'data', 'ICDAR2017_CLaMM_Large'\n",
    "    )\n",
    "    dataloaders = data_factory(\n",
    "        dataset_name='icdar',\n",
    "        root_dir=root_dir, \n",
    "        label_filepath=os.path.join(root_dir, '@ICDAR2017_CLaMM_Large.csv'),\n",
    "        transforms=SimCLREvalDataTransform,\n",
    "        mode='test',\n",
    "        batch_size=64,\n",
    "        num_cpus=8\n",
    "    )    \n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='gpu',\n",
    "        devices=-1,\n",
    "        max_epochs=1,\n",
    "        enable_progress_bar=True,\n",
    "        precision=16,\n",
    "        enable_checkpointing=False,\n",
    "        callbacks=[pl.callbacks.RichProgressBar()]\n",
    "    )\n",
    "    \n",
    "    downstream_classifier = load_best_checkpoint(\n",
    "        model_name,\n",
    "        # choice='605092',\n",
    "        base_model_name=base_model_name,\n",
    "        base_model_version=base_model_version,\n",
    "        features=num_features,\n",
    "        num_classes=13,\n",
    "    )\n",
    "    \n",
    "    trainer.test(downstream_classifier, dataloaders.get('test'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_downstream_model('SimCLR', 'SimCLRDownstream')\n",
    "# test_downstream_model('MAE', 'MAEDownstream', 1024, '614645')\n",
    "test_downstream_model('BYOL', 'BYOLDownstream', 2048, '617770')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lme",
   "language": "python",
   "name": "lme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
