{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating downstream model against pre-trained Self-Supervised models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T17:08:35.256482447Z",
     "start_time": "2023-04-24T17:08:28.225101976Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pl_bolts.models.self_supervised.simclr import SimCLREvalDataTransform\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from pl_bolts.models.self_supervised import SimCLR, BYOL\n",
    "import torchvision.transforms as T\n",
    "from lightly.models import utils\n",
    "from lightly.models.modules import masked_autoencoder\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import plotly.express as px\n",
    "import pytorch_lightning as pl\n",
    "from time import time\n",
    "from PIL import Image\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from src.models.mae.model import MAE\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_best_checkpoint(selected_model, choice=None):\n",
    "    logs_dir = os.path.join(\n",
    "        '/home/woody/iwfa/iwfa028h/dev/faps/data/trained_models/',\n",
    "        selected_model,\n",
    "        'lightning_logs'\n",
    "    )\n",
    "\n",
    "    best_version = max(\n",
    "        map(\n",
    "            lambda x: int(x.replace('version_', '')) if 'version' in x else 0,\n",
    "            os.listdir(logs_dir)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    version_dir = os.path.join(logs_dir, f'version_{best_version if not choice else choice}', 'checkpoints')\n",
    "    best_checkpoint = os.path.join(version_dir, os.listdir(version_dir)[0])\n",
    "    print('LATEST CHECKPOINT', best_checkpoint)\n",
    "\n",
    "    return best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ICDARDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_filepath, root_dir, transforms=None, convert_rgb=True):\n",
    "        \n",
    "        self.transforms = transforms\n",
    "        self.convert_rgb = convert_rgb\n",
    "        \n",
    "        df = pd.read_csv(csv_filepath, sep=';')\n",
    "        df['img_path'] = root_dir + os.sep + df.FILENAME\n",
    "        self.data = df.loc[\n",
    "            (df.img_path.map(os.path.exists)) &\n",
    "            (df.img_path.str.contains(''))\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = self.data.loc[idx, 'img_path']\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "        except Exception as ex:\n",
    "            return None\n",
    "\n",
    "        if self.convert_rgb:\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        return image, self.data.loc[idx, 'SCRIPT_TYPE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_best_checkpoint(model_name, choice=None, **model_kwargs):\n",
    "    \n",
    "    checkpoint = get_best_checkpoint(model_name, choice)\n",
    "    print(model_kwargs)\n",
    "    \n",
    "    if model_name == 'SimCLR':\n",
    "        model = SimCLR.load_from_checkpoint(checkpoint, strict=False, **model_kwargs)\n",
    "        return model.encoder\n",
    "    elif model_name == 'BYOL':\n",
    "        model = BYOL.load_from_checkpoint(checkpoint, strict=False, **model_kwargs)\n",
    "    elif model_name == 'MAE':\n",
    "        model = MAE.load_from_checkpoint(checkpoint, strict=False, **model_kwargs)\n",
    "        return model\n",
    "    elif model_name in ['SimCLRDownstream', 'MAEDownstream', 'BYOLDownstream', 'DownstreamClassifier']:\n",
    "        model = DownstreamClassifier.load_from_checkpoint(checkpoint, strict=False, **model_kwargs)\n",
    "        return model\n",
    "    else:\n",
    "        model = SimCLR.load_from_checkpoint(checkpoint, strict=False)\n",
    "        return embeddings_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_factory(dataset_name, root_dir, label_filepath, transforms, mode, batch_size, collate_fn=None, num_cpus=None):\n",
    "\n",
    "    if dataset_name.lower() == 'icdar':\n",
    "        dataset = ICDARDataset(label_filepath, root_dir, transforms=transforms(), convert_rgb=True)\n",
    "    else:\n",
    "        raise NotImplementedError(f'Dataset {dataset_name} is not implemented')\n",
    "\n",
    "    total_count = len(dataset)\n",
    "    train_count = int(0.7 * total_count)\n",
    "    val_count = int(0.1 * total_count)\n",
    "    test_count = total_count - train_count - val_count\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset,\n",
    "        (train_count, val_count, test_count),\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    if mode in 'train':\n",
    "        return {\n",
    "            'train': DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                drop_last=True,\n",
    "                pin_memory=True,\n",
    "#                 persistent_workers=False,\n",
    "                num_workers=num_cpus or os.cpu_count(),\n",
    "                collate_fn=collate_fn() if collate_fn else None\n",
    "            ),\n",
    "            'val': DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                drop_last=False,\n",
    "                pin_memory=True,\n",
    "#                 persistent_workers=False,\n",
    "                num_workers=num_cpus or os.cpu_count(),\n",
    "                collate_fn=collate_fn() if collate_fn else None\n",
    "            )\n",
    "        }\n",
    "    elif mode == 'test':\n",
    "        return {\n",
    "            'test': DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                drop_last=False,\n",
    "                pin_memory=True,\n",
    "#                 persistent_workers=False,\n",
    "                num_workers=num_cpus or os.cpu_count(),\n",
    "                collate_fn=collate_fn() if collate_fn else None\n",
    "            )\n",
    "        }\n",
    "    else:\n",
    "        raise KeyError(f'Unknown mode: {mode}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# root_dir = os.path.join(\n",
    "#     '/home/woody/iwfa/iwfa028h/dev/faps', 'data', 'ICDAR2017_CLaMM_Training'\n",
    "# )\n",
    "\n",
    "# train_dataloaders = data_factory(\n",
    "#     dataset_name='icdar',\n",
    "#     root_dir=root_dir,\n",
    "#     label_filepath=os.path.join(root_dir, '@ICDAR2017_CLaMM_Training.csv'),\n",
    "#     transforms=SimCLREvalDataTransform,\n",
    "#     mode='train',\n",
    "#     batch_size=256,\n",
    "#     num_cpus=4\n",
    "# )\n",
    "\n",
    "# test_dataloaders = data_factory(\n",
    "#     dataset_name='icdar',\n",
    "#     root_dir=root_dir, \n",
    "#     label_filepath=os.path.join(root_dir, '@ICDAR2017_CLaMM_Training.csv'),\n",
    "#     transforms=SimCLREvalDataTransform,\n",
    "#     mode='test',\n",
    "#     batch_size=256,\n",
    "#     num_cpus=4\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_features(model, data_loader, num_feats, batch_size, num_samples, perplexity=25):\n",
    "    num_samples = len(data_loader) if not num_samples else num_samples\n",
    "    feats = np.array([]).reshape((0, num_feats))\n",
    "    labels = np.array([])\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "\n",
    "    processed_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for (x1, x2, _), label in data_loader:\n",
    "            if processed_samples >= num_samples:\n",
    "                break\n",
    "            x1 = x1.squeeze().cuda()\n",
    "            out = model(x1)\n",
    "            out = out[-1].detach().cpu().numpy()\n",
    "            print(out.shape)\n",
    "            feats = np.append(feats, out, axis=0)\n",
    "            labels = np.append(labels, label, axis=0)\n",
    "            processed_samples += batch_size\n",
    "\n",
    "    tsne = TSNE(n_components=3, perplexity=perplexity, init='pca')\n",
    "    x_feats = tsne.fit_transform(feats)\n",
    "\n",
    "    dim_red_df = pd.DataFrame(x_feats)\n",
    "    dim_red_df['labels'] = pd.Categorical(labels)\n",
    "    fig = px.scatter_3d(dim_red_df, x=0, y=1, z=2, color='labels', size_max=5)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_from_embeddings(model, dataloader):\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for images, labels in dataloader:\n",
    "        x1, x2, _ = images\n",
    "        x1 = x1.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(x1)[-1].detach().cpu().numpy()\n",
    "        X.append(embeddings)\n",
    "        y.append(labels.numpy())\n",
    "        \n",
    "    X = np.concatenate(X)\n",
    "    y = np.concatenate(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cluster_embeddings():\n",
    "    \n",
    "    simclr_encoder = load_best_checkpoint('SimCLR')\n",
    "    simclr_encoder.eval()\n",
    "    simclr_encoder.cuda()\n",
    "    \n",
    "    X_train, y_train = generate_from_embeddings(simclr_encoder, train_dataloaders.get('train'))\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=10)\n",
    "    print(knn)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    X_test, y_test = generate_from_embeddings(simclr_encoder, test_dataloaders.get('test'))\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    return knn\n",
    "    \n",
    "#     plot_features(simclr_encoder, dataloaders.get('train'), 2048, 256, 1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DownstreamClassifier(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, base_model_name='SimCLR', base_model_version=None, features=2048, num_classes=13, learning_rate=3e-4):\n",
    "        print(base_model_name, features, num_classes)\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "                \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        s = time()\n",
    "        self.base_model = load_best_checkpoint(base_model_name, choice=base_model_version, num_classes=num_classes)\n",
    "        self.base_model.eval()\n",
    "        self.base_model.cuda()\n",
    "        print('Base model load time: ', time() - s)\n",
    "\n",
    "        self.classifier = nn.Linear(features, num_classes)\n",
    "        \n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.accuracy_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.num_classes).to(self.device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.base_model.forward_encoder(x, 0)\n",
    "            if isinstance(x, list):\n",
    "                x = x[-1]\n",
    "            elif isinstance(x, tuple):\n",
    "                x = x[0]\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x1, x2, _), label = batch\n",
    "        y_hat = self(x1)\n",
    "        y_hat, _ = torch.max(y_hat, dim=1)\n",
    "        loss = self.loss_fn(y_hat, label)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        (x1, x2, _), label = batch\n",
    "        y_hat = self(x1)\n",
    "        y_hat, _ = torch.max(y_hat, dim=1)\n",
    "        loss = self.loss_fn(y_hat, label)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        (x1, x2, _), label = batch\n",
    "        y_hat = self(x1)\n",
    "        y_hat, _ = torch.max(y_hat, dim=1)\n",
    "        acc = self.accuracy_fn(y_hat, label)\n",
    "        self.log('test_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=0.0008)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_downstream_model(model_name, downstream_model_name, model_version=None, feats=2048, max_epochs=10):\n",
    "    \n",
    "    root_dir = os.path.join(\n",
    "        '/home/woody/iwfa/iwfa028h/dev/faps', 'data', 'ICDAR2017_CLaMM_Large'\n",
    "    )\n",
    "    dataloaders = data_factory(\n",
    "        dataset_name='icdar',\n",
    "        root_dir=root_dir, \n",
    "        label_filepath=os.path.join(root_dir, '@ICDAR2017_CLaMM_Large.csv'),\n",
    "        transforms=SimCLREvalDataTransform,\n",
    "        mode='train',\n",
    "        batch_size=64,\n",
    "        num_cpus=8\n",
    "    )\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=os.path.abspath(os.path.join(root_dir, '..', 'trained_models', downstream_model_name)),\n",
    "        accelerator='gpu',\n",
    "        devices=-1,\n",
    "        max_epochs=max_epochs,\n",
    "        enable_progress_bar=True,\n",
    "        precision=16,\n",
    "        callbacks=[\n",
    "            pl.callbacks.ModelCheckpoint(mode=\"min\", monitor=\"val_loss\"),\n",
    "            pl.callbacks.RichProgressBar()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    downstream_classifier = DownstreamClassifier(model_name, model_version, feats, 13, 3e-3)\n",
    "    \n",
    "    trainer.fit(downstream_classifier, dataloaders.get('train'), dataloaders.get('val'))\n",
    "    \n",
    "    return downstream_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE 1024 13\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/woody/iwfa/iwfa028h/dev/faps/data/trained_models/MAE/lightning_logs/version_614805/checkpoints'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# downstream_model = train_downstream_model('SimCLR', 'SimCLRDownstream', None, 2048, 100)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m downstream_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_downstream_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMAE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMAEDownstream\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m614805\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 29\u001b[0m, in \u001b[0;36mtrain_downstream_model\u001b[0;34m(model_name, downstream_model_name, model_version, feats, max_epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m data_factory(\n\u001b[1;32m      7\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124micdar\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     root_dir\u001b[38;5;241m=\u001b[39mroot_dir, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     num_cpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     17\u001b[0m     default_root_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_models\u001b[39m\u001b[38;5;124m'\u001b[39m, downstream_model_name)),\n\u001b[1;32m     18\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     ]\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m downstream_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mDownstreamClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(downstream_classifier, dataloaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m), dataloaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m downstream_classifier\n",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m, in \u001b[0;36mDownstreamClassifier.__init__\u001b[0;34m(self, base_model_name, base_model_version, features, num_classes, learning_rate)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n\u001b[1;32m     13\u001b[0m s \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_best_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchoice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mcuda()\n",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m, in \u001b[0;36mload_best_checkpoint\u001b[0;34m(model_name, choice, **model_kwargs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_best_checkpoint\u001b[39m(model_name, choice\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs):\n\u001b[0;32m----> 3\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mget_best_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchoice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model_kwargs)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSimCLR\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m, in \u001b[0;36mget_best_checkpoint\u001b[0;34m(selected_model, choice)\u001b[0m\n\u001b[1;32m      8\u001b[0m best_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mint\u001b[39m(x\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion_\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     11\u001b[0m         os\u001b[38;5;241m.\u001b[39mlistdir(logs_dir)\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m version_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(logs_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_version\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m\u001b[38;5;250m \u001b[39mchoice\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39mchoice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoints\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m best_checkpoint \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(version_dir, \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion_dir\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLATEST CHECKPOINT\u001b[39m\u001b[38;5;124m'\u001b[39m, best_checkpoint)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_checkpoint\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/woody/iwfa/iwfa028h/dev/faps/data/trained_models/MAE/lightning_logs/version_614805/checkpoints'"
     ]
    }
   ],
   "source": [
    "# downstream_model = train_downstream_model('SimCLR', 'SimCLRDownstream', None, 2048, 100)\n",
    "downstream_model = train_downstream_model('MAE', 'MAEDownstream', '614645', 1024, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_downstream_model(base_model_name, model_name, num_features, base_model_version):\n",
    "    \n",
    "    root_dir = os.path.join(\n",
    "        '/home/woody/iwfa/iwfa028h/dev/faps', 'data', 'ICDAR2017_CLaMM_Large'\n",
    "    )\n",
    "    dataloaders = data_factory(\n",
    "        dataset_name='icdar',\n",
    "        root_dir=root_dir, \n",
    "        label_filepath=os.path.join(root_dir, '@ICDAR2017_CLaMM_Large.csv'),\n",
    "        transforms=SimCLREvalDataTransform,\n",
    "        mode='test',\n",
    "        batch_size=64,\n",
    "        num_cpus=8\n",
    "    )    \n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='gpu',\n",
    "        devices=-1,\n",
    "        max_epochs=1,\n",
    "        enable_progress_bar=True,\n",
    "        precision=16,\n",
    "        enable_checkpointing=False,\n",
    "        callbacks=[pl.callbacks.RichProgressBar()]\n",
    "    )\n",
    "    \n",
    "    downstream_classifier = load_best_checkpoint(\n",
    "        model_name,\n",
    "        # choice='605092',\n",
    "        base_model_name=base_model_name,\n",
    "        base_model_version=base_model_version,\n",
    "        features=num_features,\n",
    "        num_classes=13,\n",
    "    )\n",
    "    \n",
    "    trainer.test(downstream_classifier, dataloaders.get('test'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">Testing</span> <span style=\"color: #6206e0; text-decoration-color: #6206e0\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">18/18</span> <span style=\"color: #8a8a8a; text-decoration-color: #8a8a8a\">0:00:24 • 0:00:00</span> <span style=\"color: #b2b2b2; text-decoration-color: #b2b2b2\">0.72it/s</span>  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37mTesting\u001b[0m \u001b[38;2;98;6;224m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[37m18/18\u001b[0m \u001b[38;5;245m0:00:24 • 0:00:00\u001b[0m \u001b[38;5;249m0.72it/s\u001b[0m  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2759242653846741     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2759242653846741    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test_downstream_model('SimCLR', 'SimCLRDownstream')\n",
    "test_downstream_model('MAE', 'MAEDownstream', 1024, '614645')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lme",
   "language": "python",
   "name": "lme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
