{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating downstream model against pre-trained Self-Supervised models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T17:08:35.256482447Z",
     "start_time": "2023-04-24T17:08:28.225101976Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "There was a problem when trying to write in your cache folder (/home/jovyan/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.\n",
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:35: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:93: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pl_bolts/losses/self_supervised_learning.py:234: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n",
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pl_bolts/datamodules/experience_source.py:18: UnderReviewWarning: The feature warn_missing_pkg is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  warn_missing_pkg(\"gym\")\n",
      "Matplotlib created a temporary config/cache directory at /tmp/613174.tinygpu/matplotlib-_6m6u1xc because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "from pl_bolts.models.self_supervised.simclr import SimCLREvalDataTransform\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from pl_bolts.models.self_supervised import SimCLR, BYOL\n",
    "import torchvision.transforms as T\n",
    "from lightly.models import utils\n",
    "from lightly.models.modules import masked_autoencoder\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "import plotly.express as px\n",
    "import pytorch_lightning as pl\n",
    "from time import time\n",
    "from PIL import Image\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from src.models.mae.model import MAE\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_best_checkpoint(selected_model, choice=None):\n",
    "    logs_dir = os.path.join(\n",
    "        '/home/woody/iwfa/iwfa028h/dev/faps/data/trained_models/',\n",
    "        selected_model,\n",
    "        'lightning_logs'\n",
    "    )\n",
    "\n",
    "    best_version = max(\n",
    "        map(\n",
    "            lambda x: int(x.replace('version_', '')) if 'version' in x else 0,\n",
    "            os.listdir(logs_dir)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    version_dir = os.path.join(logs_dir, f'version_{best_version if not choice else choice}', 'checkpoints')\n",
    "    best_checkpoint = os.path.join(version_dir, os.listdir(version_dir)[0])\n",
    "    print('LATEST CHECKPOINT', best_checkpoint)\n",
    "\n",
    "    return best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ICDARDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_filepath, root_dir, transforms=None, convert_rgb=True):\n",
    "        \n",
    "        self.transforms = transforms\n",
    "        self.convert_rgb = convert_rgb\n",
    "        \n",
    "        df = pd.read_csv(csv_filepath, sep=';')\n",
    "        df['img_path'] = root_dir + os.sep + df.FILENAME\n",
    "        self.data = df.loc[\n",
    "            (df.img_path.map(os.path.exists)) &\n",
    "            (df.img_path.str.contains(''))\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = self.data.loc[idx, 'img_path']\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "        except Exception as ex:\n",
    "            return None\n",
    "\n",
    "        if self.convert_rgb:\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        return image, self.data.loc[idx, 'SCRIPT_TYPE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_best_checkpoint(model_name, choice=None, **model_kwargs):\n",
    "    \n",
    "    checkpoint = get_best_checkpoint(model_name, choice)\n",
    "    print(model_kwargs)\n",
    "    \n",
    "    if model_name == 'SimCLR':\n",
    "        model = SimCLR.load_from_checkpoint(checkpoint, strict=False, **model_kwargs)\n",
    "        return model.encoder\n",
    "    elif model_name == 'BYOL':\n",
    "        model = BYOL.load_from_checkpoint(checkpoint, strict=False, **model_kwargs)\n",
    "    elif model_name == 'MAE':\n",
    "        model = MAE.load_from_checkpoint(checkpoint, strict=False, **model_kwargs)\n",
    "        return model\n",
    "    elif model_name in ['SimCLRDownstream', 'MAEDownstream', 'BYOLDownstream', 'DownstreamClassifier']:\n",
    "        model = DownstreamClassifier.load_from_checkpoint(checkpoint, strict=False, **model_kwargs)\n",
    "        return model\n",
    "    else:\n",
    "        model = SimCLR.load_from_checkpoint(checkpoint, strict=False)\n",
    "        return embeddings_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_factory(dataset_name, root_dir, label_filepath, transforms, mode, batch_size, collate_fn=None, num_cpus=None):\n",
    "\n",
    "    if dataset_name.lower() == 'icdar':\n",
    "        dataset = ICDARDataset(label_filepath, root_dir, transforms=transforms(), convert_rgb=True)\n",
    "    else:\n",
    "        raise NotImplementedError(f'Dataset {dataset_name} is not implemented')\n",
    "\n",
    "    total_count = len(dataset)\n",
    "    train_count = int(0.7 * total_count)\n",
    "    val_count = int(0.1 * total_count)\n",
    "    test_count = total_count - train_count - val_count\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset,\n",
    "        (train_count, val_count, test_count),\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    if mode in 'train':\n",
    "        return {\n",
    "            'train': DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                drop_last=True,\n",
    "                pin_memory=True,\n",
    "                persistent_workers=True,\n",
    "                num_workers=num_cpus or os.cpu_count(),\n",
    "                collate_fn=collate_fn() if collate_fn else None\n",
    "            ),\n",
    "            'val': DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                drop_last=False,\n",
    "                pin_memory=True,\n",
    "                persistent_workers=True,\n",
    "                num_workers=num_cpus or os.cpu_count(),\n",
    "                collate_fn=collate_fn() if collate_fn else None\n",
    "            )\n",
    "        }\n",
    "    elif mode == 'test':\n",
    "        return {\n",
    "            'test': DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                drop_last=False,\n",
    "                pin_memory=True,\n",
    "                persistent_workers=True,\n",
    "                num_workers=num_cpus or os.cpu_count(),\n",
    "                collate_fn=collate_fn() if collate_fn else None\n",
    "            )\n",
    "        }\n",
    "    else:\n",
    "        raise KeyError(f'Unknown mode: {mode}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# root_dir = os.path.join(\n",
    "#     '/home/woody/iwfa/iwfa028h/dev/faps', 'data', 'ICDAR2017_CLaMM_Training'\n",
    "# )\n",
    "\n",
    "# train_dataloaders = data_factory(\n",
    "#     dataset_name='icdar',\n",
    "#     root_dir=root_dir,\n",
    "#     label_filepath=os.path.join(root_dir, '@ICDAR2017_CLaMM_Training.csv'),\n",
    "#     transforms=SimCLREvalDataTransform,\n",
    "#     mode='train',\n",
    "#     batch_size=256,\n",
    "#     num_cpus=4\n",
    "# )\n",
    "\n",
    "# test_dataloaders = data_factory(\n",
    "#     dataset_name='icdar',\n",
    "#     root_dir=root_dir, \n",
    "#     label_filepath=os.path.join(root_dir, '@ICDAR2017_CLaMM_Training.csv'),\n",
    "#     transforms=SimCLREvalDataTransform,\n",
    "#     mode='test',\n",
    "#     batch_size=256,\n",
    "#     num_cpus=4\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_features(model, data_loader, num_feats, batch_size, num_samples, perplexity=25):\n",
    "    num_samples = len(data_loader) if not num_samples else num_samples\n",
    "    feats = np.array([]).reshape((0, num_feats))\n",
    "    labels = np.array([])\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "\n",
    "    processed_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for (x1, x2, _), label in data_loader:\n",
    "            if processed_samples >= num_samples:\n",
    "                break\n",
    "            x1 = x1.squeeze().cuda()\n",
    "            out = model(x1)\n",
    "            out = out[-1].detach().cpu().numpy()\n",
    "            print(out.shape)\n",
    "            feats = np.append(feats, out, axis=0)\n",
    "            labels = np.append(labels, label, axis=0)\n",
    "            processed_samples += batch_size\n",
    "\n",
    "    tsne = TSNE(n_components=3, perplexity=perplexity, init='pca')\n",
    "    x_feats = tsne.fit_transform(feats)\n",
    "\n",
    "    dim_red_df = pd.DataFrame(x_feats)\n",
    "    dim_red_df['labels'] = pd.Categorical(labels)\n",
    "    fig = px.scatter_3d(dim_red_df, x=0, y=1, z=2, color='labels', size_max=5)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_from_embeddings(model, dataloader):\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for images, labels in dataloader:\n",
    "        x1, x2, _ = images\n",
    "        x1 = x1.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(x1)[-1].detach().cpu().numpy()\n",
    "        X.append(embeddings)\n",
    "        y.append(labels.numpy())\n",
    "        \n",
    "    X = np.concatenate(X)\n",
    "    y = np.concatenate(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cluster_embeddings():\n",
    "    \n",
    "    simclr_encoder = load_best_checkpoint('SimCLR')\n",
    "    simclr_encoder.eval()\n",
    "    simclr_encoder.cuda()\n",
    "    \n",
    "    X_train, y_train = generate_from_embeddings(simclr_encoder, train_dataloaders.get('train'))\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=10)\n",
    "    print(knn)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    X_test, y_test = generate_from_embeddings(simclr_encoder, test_dataloaders.get('test'))\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    return knn\n",
    "    \n",
    "#     plot_features(simclr_encoder, dataloaders.get('train'), 2048, 256, 1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DownstreamClassifier(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, base_model_name='SimCLR', base_model_version=None, features=2048, num_classes=13, learning_rate=3e-4):\n",
    "        print(base_model_name, features, num_classes)\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "                \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.base_model = load_best_checkpoint(base_model_name, choice=base_model_version, num_classes=num_classes, decoder_embed_dim=102)\n",
    "        self.base_model.eval()\n",
    "\n",
    "        self.classifier = nn.Linear(features, num_classes)\n",
    "        \n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.accuracy_fn = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.num_classes).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate, weight_decay=0.0008)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.base_model.forward_encoder(x, 0)\n",
    "            if isinstance(x, list):\n",
    "                x = x[-1]\n",
    "            elif isinstance(x, tuple):\n",
    "                x = x[0]\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x1, x2, _), label = batch\n",
    "        y_hat = self(x1)\n",
    "        y_hat, _ = torch.max(y_hat, dim=1)\n",
    "        loss = self.loss_fn(y_hat, label)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        (x1, x2, _), label = batch\n",
    "        y_hat = self(x1)\n",
    "        y_hat, _ = torch.max(y_hat, dim=1)\n",
    "        loss = self.loss_fn(y_hat, label)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        (x1, x2, _), label = batch\n",
    "        y_hat = self(x1)\n",
    "        y_hat, _ = torch.max(y_hat, dim=1)\n",
    "        acc = self.accuracy_fn(y_hat, label)\n",
    "        self.log('test_acc', acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_downstream_model(model_name, downstream_model_name, model_version=None, feats=2048, max_epochs=10):\n",
    "    \n",
    "    root_dir = os.path.join(\n",
    "        '/home/woody/iwfa/iwfa028h/dev/faps', 'data', 'ICDAR2017_CLaMM_task1_task3'\n",
    "    )\n",
    "    dataloaders = data_factory(\n",
    "        dataset_name='icdar',\n",
    "        root_dir=root_dir, \n",
    "        label_filepath=os.path.join(root_dir, '@ICDAR2017_CLaMM_task1_task3.csv'),\n",
    "        transforms=SimCLREvalDataTransform,\n",
    "        mode='train',\n",
    "        batch_size=64,\n",
    "        num_cpus=8\n",
    "    )\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=os.path.abspath(os.path.join(root_dir, '..', 'trained_models', downstream_model_name)),\n",
    "        accelerator='gpu',\n",
    "        devices=-1,\n",
    "        max_epochs=max_epochs,\n",
    "        enable_progress_bar=True,\n",
    "        precision=16,\n",
    "        callbacks=[\n",
    "            pl.callbacks.ModelCheckpoint(mode=\"min\", monitor=\"val_loss\"),\n",
    "            pl.callbacks.RichProgressBar()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    downstream_classifier = DownstreamClassifier(model_name, model_version, feats, 13)\n",
    "    \n",
    "    trainer.fit(downstream_classifier, dataloaders.get('train'), dataloaders.get('val'))\n",
    "    \n",
    "    return downstream_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE 1024 13\n",
      "LATEST CHECKPOINT /home/woody/iwfa/iwfa028h/dev/faps/data/trained_models/MAE/lightning_logs/version_594344/checkpoints/epoch=494-step=9405.ckpt\n",
      "{'num_classes': 13}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MAE:\n\tsize mismatch for mask_token: copying a param with shape torch.Size([1, 1, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# downstream_model = train_downstream_model('SimCLR', 'SimCLRDownstream', None, 2048, 100)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m downstream_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_downstream_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMAE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMAEDownstream\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m594344\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 29\u001b[0m, in \u001b[0;36mtrain_downstream_model\u001b[0;34m(model_name, downstream_model_name, model_version, feats, max_epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m data_factory(\n\u001b[1;32m      7\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124micdar\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     root_dir\u001b[38;5;241m=\u001b[39mroot_dir, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     num_cpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     17\u001b[0m     default_root_dir\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrained_models\u001b[39m\u001b[38;5;124m'\u001b[39m, downstream_model_name)),\n\u001b[1;32m     18\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     ]\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m downstream_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mDownstreamClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(downstream_classifier, dataloaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m), dataloaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m downstream_classifier\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mDownstreamClassifier.__init__\u001b[0;34m(self, base_model_name, base_model_version, features, num_classes, learning_rate)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate \u001b[38;5;241m=\u001b[39m learning_rate\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes \u001b[38;5;241m=\u001b[39m num_classes\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_best_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchoice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_model_version\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(features, num_classes)\n",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m, in \u001b[0;36mload_best_checkpoint\u001b[0;34m(model_name, choice, **model_kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m     model \u001b[38;5;241m=\u001b[39m BYOL\u001b[38;5;241m.\u001b[39mload_from_checkpoint(checkpoint, strict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 12\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mMAE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSimCLRDownstream\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAEDownstream\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBYOLDownstream\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownstreamClassifier\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/.conda/envs/lme/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:136\u001b[0m, in \u001b[0;36mModelIO.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     64\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningDataModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/lme/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:179\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/lme/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:237\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# load the state_dict on the model automatically\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m strict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m strict:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m keys\u001b[38;5;241m.\u001b[39mmissing_keys:\n",
      "File \u001b[0;32m~/.conda/envs/lme/lib/python3.10/site-packages/torch/nn/modules/module.py:1667\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1662\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   1663\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1664\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1668\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MAE:\n\tsize mismatch for mask_token: copying a param with shape torch.Size([1, 1, 1024]) from checkpoint, the shape in current model is torch.Size([1, 1, 512])."
     ]
    }
   ],
   "source": [
    "# downstream_model = train_downstream_model('SimCLR', 'SimCLRDownstream', None, 2048, 100)\n",
    "downstream_model = train_downstream_model('MAE', 'MAEDownstream', '594344', 1024, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_downstream_model(base_model_name, model_name, num_features, base_model_version):\n",
    "    \n",
    "    root_dir = os.path.join(\n",
    "        '/home/woody/iwfa/iwfa028h/dev/faps', 'data', 'ICDAR2017_CLaMM_task1_task3'\n",
    "    )\n",
    "    dataloaders = data_factory(\n",
    "        dataset_name='icdar',\n",
    "        root_dir=root_dir, \n",
    "        label_filepath=os.path.join(root_dir, '@ICDAR2017_CLaMM_task1_task3.csv'),\n",
    "        transforms=SimCLREvalDataTransform,\n",
    "        mode='test',\n",
    "        batch_size=64,\n",
    "        num_cpus=8\n",
    "    )    \n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='gpu',\n",
    "        devices=-1,\n",
    "        max_epochs=1,\n",
    "        enable_progress_bar=True,\n",
    "        precision=16,\n",
    "        enable_checkpointing=False,\n",
    "        callbacks=[pl.callbacks.RichProgressBar()]\n",
    "    )\n",
    "    \n",
    "    downstream_classifier = load_best_checkpoint(\n",
    "        model_name,\n",
    "        # choice='605092',\n",
    "        base_model_name=base_model_name,\n",
    "        base_model_version=base_model_version,\n",
    "        features=num_features,\n",
    "        num_classes=13,\n",
    "    )\n",
    "    \n",
    "    trainer.test(downstream_classifier, dataloaders.get('test'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATEST CHECKPOINT /home/woody/iwfa/iwfa028h/dev/faps/data/trained_models/MAEDownstream/lightning_logs/version_612365/checkpoints/epoch=3-step=84.ckpt\n",
      "{'base_model_name': 'MAE', 'base_model_version': 606373, 'features': 1024, 'num_classes': 13}\n",
      "MAE 1024 13\n",
      "LATEST CHECKPOINT /home/woody/iwfa/iwfa028h/dev/faps/data/trained_models/MAE/lightning_logs/version_606373/checkpoints/epoch=98-step=1485.ckpt\n",
      "{'num_classes': 13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558b798a0e5141bcbffc6b190a60607d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: \n",
       "UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current\n",
       "system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker \n",
       "creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential \n",
       "slowness/freeze if necessary.\n",
       "  warnings.warn(_create_warning_msg(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/torch/utils/data/dataloader.py:554: \n",
       "UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current\n",
       "system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker \n",
       "creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential \n",
       "slowness/freeze if necessary.\n",
       "  warnings.warn(_create_warning_msg(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2219451367855072     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2219451367855072    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test_downstream_model('SimCLR', 'SimCLRDownstream')\n",
    "test_downstream_model('MAE', 'MAEDownstream', 1024, '606373')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lme",
   "language": "python",
   "name": "lme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
