{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating downstream model against pre-trained Self-Supervised models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T17:08:35.256482447Z",
     "start_time": "2023-04-24T17:08:28.225101976Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:35: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:93: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pl_bolts/losses/self_supervised_learning.py:234: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n",
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pl_bolts/datamodules/experience_source.py:18: UnderReviewWarning: The feature warn_missing_pkg is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  warn_missing_pkg(\"gym\")\n"
     ]
    }
   ],
   "source": [
    "from pl_bolts.models.self_supervised.simclr import SimCLREvalDataTransform\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from pl_bolts.models.self_supervised import SimCLR, BYOL\n",
    "# from livelossplot import PlotLosses\n",
    "import torchvision.transforms as T\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score\n",
    "import plotly.express as px\n",
    "import pytorch_lightning as pl\n",
    "from time import time\n",
    "from PIL import Image\n",
    "import torchmetrics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_checkpoint(selected_model):\n",
    "    logs_dir = os.path.join(\n",
    "        '/home/woody/iwfa/iwfa028h/dev/faps/data/trained_models/',\n",
    "        selected_model,\n",
    "        'lightning_logs'\n",
    "    )\n",
    "\n",
    "    best_version = max(\n",
    "        map(\n",
    "            lambda x: int(x.replace('version_', '')) if 'version' in x else 0,\n",
    "            os.listdir(logs_dir)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    version_dir = os.path.join(logs_dir, f'version_{best_version}', 'checkpoints')\n",
    "    best_checkpoint = os.path.join(version_dir, os.listdir(version_dir)[0])\n",
    "    print('LATEST CHECKPOINT', best_checkpoint)\n",
    "\n",
    "    return best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ICDARDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_filepath, root_dir, transforms=None, convert_rgb=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "        self.data = pd.read_csv(csv_filepath, sep=';')\n",
    "        self.convert_rgb = convert_rgb\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = os.path.join(self.root_dir, self.data.loc[idx, 'FILENAME'])\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "        except Exception as ex:\n",
    "            return None\n",
    "\n",
    "        if self.convert_rgb:\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        return image, self.data.loc[idx, 'SCRIPT_TYPE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_checkpoint(model_name, **model_kwargs):\n",
    "    \n",
    "    checkpoint = get_best_checkpoint(model_name)\n",
    "    print(model_kwargs)\n",
    "    \n",
    "    if model_name == 'SimCLR':\n",
    "        model = SimCLR.load_from_checkpoint(checkpoint, strict=False, **model_kwargs)\n",
    "        return model.encoder\n",
    "    elif model_name == 'SimCLRDownstream':\n",
    "        model = DownstreamClassifier.load_from_checkpoint(checkpoint, strict=False, **model_kwargs)\n",
    "        return model\n",
    "    else:\n",
    "        model = SimCLR.load_from_checkpoint(checkpoint, strict=False)\n",
    "        return embeddings_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_factory(dataset_name, root_dir, label_filepath, transforms, mode, batch_size, collate_fn=None, num_cpus=None):\n",
    "\n",
    "    if dataset_name.lower() == 'icdar':\n",
    "        dataset = ICDARDataset(label_filepath, root_dir, transforms=transforms(), convert_rgb=True)\n",
    "    else:\n",
    "        raise NotImplementedError(f'Dataset {dataset_name} is not implemented')\n",
    "\n",
    "    total_count = len(dataset)\n",
    "    train_count = int(0.7 * total_count)\n",
    "    val_count = int(0.2 * total_count)\n",
    "    test_count = total_count - train_count - val_count\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = random_split(\n",
    "        dataset,\n",
    "        (train_count, val_count, test_count),\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "    )\n",
    "\n",
    "    if mode in 'train':\n",
    "        return {\n",
    "            'train': DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                drop_last=True,\n",
    "                pin_memory=True,\n",
    "                persistent_workers=True,\n",
    "                num_workers=num_cpus or os.cpu_count(),\n",
    "                collate_fn=collate_fn() if collate_fn else None\n",
    "            ),\n",
    "            'val': DataLoader(\n",
    "                val_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                drop_last=False,\n",
    "                pin_memory=True,\n",
    "                persistent_workers=True,\n",
    "                num_workers=num_cpus or os.cpu_count(),\n",
    "                collate_fn=collate_fn() if collate_fn else None\n",
    "            )\n",
    "        }\n",
    "    elif mode == 'test':\n",
    "        return {\n",
    "            'test': DataLoader(\n",
    "                test_dataset,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,\n",
    "                drop_last=False,\n",
    "                pin_memory=True,\n",
    "                persistent_workers=True,\n",
    "                num_workers=num_cpus or os.cpu_count(),\n",
    "                collate_fn=collate_fn() if collate_fn else None\n",
    "            )\n",
    "        }\n",
    "    else:\n",
    "        raise KeyError(f'Unknown mode: {mode}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_dir = os.path.join(\n",
    "#     '/home/woody/iwfa/iwfa028h/dev/faps', 'data', 'ICDAR2017_CLaMM_Training'\n",
    "# )\n",
    "\n",
    "# train_dataloaders = data_factory(\n",
    "#     dataset_name='icdar',\n",
    "#     root_dir=root_dir,\n",
    "#     label_filepath=os.path.join(root_dir, '@ICDAR2017_CLaMM_Training.csv'),\n",
    "#     transforms=SimCLREvalDataTransform,\n",
    "#     mode='train',\n",
    "#     batch_size=256,\n",
    "#     num_cpus=4\n",
    "# )\n",
    "\n",
    "# test_dataloaders = data_factory(\n",
    "#     dataset_name='icdar',\n",
    "#     root_dir=root_dir, \n",
    "#     label_filepath=os.path.join(root_dir, '@ICDAR2017_CLaMM_Training.csv'),\n",
    "#     transforms=SimCLREvalDataTransform,\n",
    "#     mode='test',\n",
    "#     batch_size=256,\n",
    "#     num_cpus=4\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(model, data_loader, num_feats, batch_size, num_samples, perplexity=25):\n",
    "    num_samples = len(data_loader) if not num_samples else num_samples\n",
    "    feats = np.array([]).reshape((0, num_feats))\n",
    "    labels = np.array([])\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "\n",
    "    processed_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for (x1, x2, _), label in data_loader:\n",
    "            if processed_samples >= num_samples:\n",
    "                break\n",
    "            x1 = x1.squeeze().cuda()\n",
    "            out = model(x1)\n",
    "            out = out[-1].detach().cpu().numpy()\n",
    "            print(out.shape)\n",
    "            feats = np.append(feats, out, axis=0)\n",
    "            labels = np.append(labels, label, axis=0)\n",
    "            processed_samples += batch_size\n",
    "\n",
    "    tsne = TSNE(n_components=3, perplexity=perplexity, init='pca')\n",
    "    x_feats = tsne.fit_transform(feats)\n",
    "\n",
    "    dim_red_df = pd.DataFrame(x_feats)\n",
    "    dim_red_df['labels'] = pd.Categorical(labels)\n",
    "    fig = px.scatter_3d(dim_red_df, x=0, y=1, z=2, color='labels', size_max=5)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_embeddings(model, dataloader):\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for images, labels in dataloader:\n",
    "        x1, x2, _ = images\n",
    "        x1 = x1.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(x1)[-1].detach().cpu().numpy()\n",
    "        X.append(embeddings)\n",
    "        y.append(labels.numpy())\n",
    "        \n",
    "    X = np.concatenate(X)\n",
    "    y = np.concatenate(y)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_embeddings():\n",
    "    \n",
    "    simclr_encoder = load_best_checkpoint('SimCLR')\n",
    "    simclr_encoder.eval()\n",
    "    simclr_encoder.cuda()\n",
    "    \n",
    "    X_train, y_train = generate_from_embeddings(simclr_encoder, train_dataloaders.get('train'))\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=10)\n",
    "    print(knn)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    X_test, y_test = generate_from_embeddings(simclr_encoder, test_dataloaders.get('test'))\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    return knn\n",
    "    \n",
    "#     plot_features(simclr_encoder, dataloaders.get('train'), 2048, 256, 1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownstreamClassifier(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, base_model_name='SimCLR', features=2048, num_classes=13, learning_rate=1e-2):\n",
    "        print(base_model_name, features, num_classes)\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "                \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.base_model = load_best_checkpoint(base_model_name, num_classes=num_classes)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(features, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.base_model(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x1, x2, _), label = batch\n",
    "        y_hat = self(x1)\n",
    "        loss = torch.nn.CrossEntropyLoss()(y_hat, label)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        (x1, x2, _), label = batch\n",
    "        y_hat = self(x1)\n",
    "        loss = torch.nn.CrossEntropyLoss()(y_hat, label)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        (x1, x2, _), label = batch\n",
    "        y_hat = self(x1)\n",
    "        acc_metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.num_classes).to(self.device)\n",
    "        acc = acc_metric(y_hat, label)\n",
    "        self.log('test_acc', acc, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_downstream_model(model_name, max_epochs=10):\n",
    "    \n",
    "    root_dir = os.path.join(\n",
    "        '/home/woody/iwfa/iwfa028h/dev/faps', 'data', 'ICDAR2017_CLaMM_Training'\n",
    "    )\n",
    "    dataloaders = data_factory(\n",
    "        dataset_name='icdar',\n",
    "        root_dir=root_dir, \n",
    "        label_filepath=os.path.join(root_dir, '@ICDAR2017_CLaMM_Training.csv'),\n",
    "        transforms=SimCLREvalDataTransform,\n",
    "        mode='train',\n",
    "        batch_size=64,\n",
    "        num_cpus=4\n",
    "    )\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=os.path.abspath(os.path.join(root_dir, '..', 'trained_models', 'SimCLRDownstream')),\n",
    "        accelerator='gpu',\n",
    "        devices=-1,\n",
    "        max_epochs=max_epochs,\n",
    "        enable_progress_bar=True,\n",
    "        precision=16,\n",
    "        callbacks=[\n",
    "            pl.callbacks.ModelCheckpoint(mode=\"min\", monitor=\"val_loss\"),\n",
    "            pl.callbacks.RichProgressBar()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    downstream_classifier = DownstreamClassifier(model_name, 2048, 13)\n",
    "    \n",
    "    trainer.fit(downstream_classifier, dataloaders.get('train'), dataloaders.get('val'))\n",
    "    \n",
    "    return downstream_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimCLR 2048 13\n",
      "LATEST CHECKPOINT /home/woody/iwfa/iwfa028h/dev/faps/data/trained_models/SimCLR/lightning_logs/version_584360/checkpoints/epoch=474-step=4275.ckpt\n",
      "{'num_classes': 13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:224: UnderReviewWarning: The feature SimCLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  obj = cls(**_cls_kwargs)\n",
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pl_bolts/models/self_supervised/simclr/simclr_module.py:138: UnderReviewWarning: The feature resnet50 is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  return backbone(first_conv=self.first_conv, maxpool1=self.maxpool1, return_all_feature_maps=False)\n",
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pl_bolts/models/self_supervised/resnets.py:321: UnderReviewWarning: The feature _resnet is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  return _resnet(\"resnet50\", Bottleneck, [3, 4, 6, 3], pretrained, progress, **kwargs)\n",
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pl_bolts/models/self_supervised/resnets.py:281: UnderReviewWarning: The feature ResNet is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  model = ResNet(block, layers, **kwargs)\n",
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pl_bolts/models/self_supervised/resnets.py:223: UnderReviewWarning: The feature conv1x1 is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  conv1x1(self.inplanes, planes * block.expansion, stride),\n",
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pl_bolts/models/self_supervised/resnets.py:229: UnderReviewWarning: The feature Bottleneck is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  block(\n",
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pl_bolts/models/self_supervised/resnets.py:110: UnderReviewWarning: The feature conv3x3 is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
      "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pl_bolts/models/self_supervised/simclr/simclr_module.py:126: UnderReviewWarning: The feature Projection is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.projection = Projection(input_dim=self.hidden_mlp, hidden_dim=self.hidden_mlp, output_dim=self.feat_dim)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ base_model │ ResNet     │ 25.6 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ classifier │ Sequential │ 26.6 K │\n",
       "└───┴────────────┴────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ base_model │ ResNet     │ 25.6 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ classifier │ Sequential │ 26.6 K │\n",
       "└───┴────────────┴────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 25.6 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 25.6 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 51                                                                         \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 25.6 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 25.6 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 51                                                                         \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a48456caea4b63b9562eb30f074a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1555: \n",
       "PossibleUserWarning: The number of training batches (38) is smaller than the logging interval \n",
       "Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training \n",
       "epoch.\n",
       "  rank_zero_warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1555: \n",
       "PossibleUserWarning: The number of training batches (38) is smaller than the logging interval \n",
       "Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training \n",
       "epoch.\n",
       "  rank_zero_warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:48: \n",
       "UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
       "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/hpc/iwfa/iwfa028h/.conda/envs/lme/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:48: \n",
       "UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
       "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "downstream_model = train_downstream_model('SimCLR', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_downstream_model(model_name):\n",
    "    \n",
    "    root_dir = os.path.join(\n",
    "        '/home/woody/iwfa/iwfa028h/dev/faps', 'data', 'ICDAR2017_CLaMM_Training'\n",
    "    )\n",
    "    dataloaders = data_factory(\n",
    "        dataset_name='icdar',\n",
    "        root_dir=root_dir, \n",
    "        label_filepath=os.path.join(root_dir, '@ICDAR2017_CLaMM_Training.csv'),\n",
    "        transforms=SimCLREvalDataTransform,\n",
    "        mode='test',\n",
    "        batch_size=64,\n",
    "        num_cpus=4\n",
    "    )\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='gpu',\n",
    "        devices=-1,\n",
    "        max_epochs=1,\n",
    "        enable_progress_bar=True,\n",
    "        precision=16,\n",
    "        enable_checkpointing=False,\n",
    "        callbacks=[pl.callbacks.RichProgressBar()]\n",
    "    )\n",
    "    \n",
    "    downstream_classifier = load_best_checkpoint(model_name, base_model_name='MAE', features=2048, num_classes=13)\n",
    "    \n",
    "    trainer.test(downstream_classifier, dataloaders.get('test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LATEST CHECKPOINT /home/woody/iwfa/iwfa028h/dev/faps/data/trained_models/SimCLRDownstream/lightning_logs/version_595294/checkpoints/epoch=5-step=228.ckpt\n",
      "{'base_model_name': 'MAE', 'features': 2048, 'num_classes': 13}\n",
      "MAE 2048 13\n",
      "LATEST CHECKPOINT /home/woody/iwfa/iwfa028h/dev/faps/data/trained_models/MAE/lightning_logs/version_594369/checkpoints/epoch=88-step=1691.ckpt\n",
      "{'num_classes': 13}\n"
     ]
    }
   ],
   "source": [
    "test_downstream_model('SimCLRDownstream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lme",
   "language": "python",
   "name": "lme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
